{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ayoung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pre_processing.get_books import get_preprocessed_data\n",
    "from pre_processing.frequency_inverse import get_freq_inverse\n",
    "from pre_processing.get_all_words import get_all_words\n",
    "\n",
    "from analysis.k_means import k_means_analysis\n",
    "\n",
    "\n",
    "from analysis.cosine_similarity import get_cosine_similarity\n",
    "from sklearn.manifold import MDS\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "from analysis.multidimensional_scaling import get_multi_scaling_positions\n",
    "from analysis.multidimensional_scaling import get_LSA_scaling_positions\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from gensim import corpora, models, similarities \n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "NUMBERS_ONLY = False\n",
    "\n",
    "IF_MIN = 0.1\n",
    "IF_MAX = 0.9\n",
    "N_GRAMS = 3\n",
    "\n",
    "K_MEANS_N_CLUSTERS = 6\n",
    "\n",
    "UseMDS=True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_books = get_preprocessed_data(NUMBERS_ONLY)\n",
    "frequency_term_matrix, terms, term_freq_vectorizer = get_freq_inverse(list_of_books,IF_MAX,IF_MIN,N_GRAMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoLarsIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "book_words_list = []\n",
    "all_words_list = []\n",
    "for book in list_of_books:\n",
    "    book_words_list.append(\" \".join(book.word_list))\n",
    "\n",
    "for book_words in book_words_list:\n",
    "    words = [word.lower() for sentence in nltk.sent_tokenize(book_words) for word in nltk.word_tokenize(sentence)]\n",
    "    filtered_tokens = []\n",
    "    for word in words:\n",
    "        filtered_tokens.append(word)\n",
    "    all_words_list.extend(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=K_MEANS_N_CLUSTERS)\n",
    "km.fit(frequency_term_matrix)\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "\n",
    "titles = []\n",
    "published = []\n",
    "authors = []\n",
    "period = []\n",
    "for book in list_of_books:\n",
    "    authors.append(book.meta[\"author\"])\n",
    "    titles.append(book.meta[\"title\"])\n",
    "    published.append(book.meta[\"published\"])\n",
    "    period.append(book.meta[\"period\"])\n",
    "\n",
    "books = {'titles': titles, 'cluster': clusters, 'published': published, 'authors':authors, 'period':period}\n",
    "frame = pd.DataFrame(books, index = [clusters] , columns = ['titles', 'cluster', 'published','authors', 'period']).sort_index()\n",
    "frame.to_csv(\"kmeansresults.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_frame = pd.DataFrame({'words': all_words_list}, index=all_words_list)\n",
    "cluster_names = {}\n",
    "for i in range(K_MEANS_N_CLUSTERS):\n",
    "    cluster_names[i] = []\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "if not NUMBERS_ONLY:\n",
    "    for i in range(K_MEANS_N_CLUSTERS):\n",
    "            for ind in order_centroids[i, :K_MEANS_N_CLUSTERS]:  # replace 6 with n words per cluster\n",
    "                cluster_names[i].append(vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0])\n",
    "else:\n",
    "    for i in range(K_MEANS_N_CLUSTERS):\n",
    "        #print(\"Cluster {} words:\".format(i))\n",
    "        for ind in order_centroids[i, :K_MEANS_N_CLUSTERS]:  # replace 6 with n words per cluster\n",
    "            cluster_names[i].append(vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posMDS = get_multi_scaling_positions(frequency_term_matrix, False)\n",
    "\n",
    "posLSA = get_LSA_scaling_positions(frequency_term_matrix)\n",
    "\n",
    "if UseMDS:\n",
    "    pos = posMDS\n",
    "    \n",
    "else:\n",
    "    pos = posLSA\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "graph_dict = {'x':posMDS[:, 0], 'y':posMDS[:, 1], 'label':clusters, 'title':titles}\n",
    "df = pd.DataFrame(graph_dict)\n",
    "\n",
    "groups = df.groupby('label')\n",
    "#posMDS\n",
    "\n",
    "ig, ax = plt.subplots(figsize=(16, 15))  # set size\n",
    "ax.margins(0.05)  # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12,\n",
    "            label=cluster_names[name][0:5],\n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params( \\\n",
    "        axis='x',  # changes apply to the x-axis\n",
    "        which='both',  # both major and minor ticks are affected\n",
    "    )\n",
    "    ax.tick_params( \\\n",
    "        axis='y',  # changes apply to the y-axis\n",
    "        which='both',  # both major and minor ticks are affected\n",
    "       )\n",
    "\n",
    "    ax.legend(numpoints=1)  # show legend with only 1 point\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(0.85,0.93))\n",
    "\n",
    "\n",
    "# plt.imshow(Z, interpolation='nearest',\n",
    "#            extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "#            cmap=plt.cm.Paired,\n",
    "#            aspect='auto', origin='lower')\n",
    "\n",
    "# add label in x,y position with the label as the book title\n",
    "\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=11)\n",
    "\n",
    "df.ix[2]\n",
    "ig.savefig('kmeans6topics.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#posLSA\n",
    "graph_dict = {'x':posLSA[:, 0], 'y':posLSA[:, 1], 'label':clusters, 'title':titles}\n",
    "df = pd.DataFrame(graph_dict)\n",
    "\n",
    "groups = df.groupby('label')\n",
    "ig, ax = plt.subplots(figsize=(23, 13))  # set size\n",
    "ax.margins(0.05)  # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12,\n",
    "            label=cluster_names[name][0:5],\n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params( \\\n",
    "        axis='x',  # changes apply to the x-axis\n",
    "        which='both',  # both major and minor ticks are affected\n",
    "    )\n",
    "    ax.tick_params( \\\n",
    "        axis='y',  # changes apply to the y-axis\n",
    "        which='both',  # both major and minor ticks are affected\n",
    "       )\n",
    "\n",
    "    ax.legend(numpoints=1)  # show legend with only 1 point\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1,0.93))\n",
    "\n",
    "\n",
    " \n",
    "# plt.imshow(Z, interpolation='nearest',\n",
    "#            extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "#            cmap=plt.cm.Paired,\n",
    "#            aspect='auto', origin='lower')\n",
    "\n",
    "# add label in x,y position with the label as the book title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=11)\n",
    "\n",
    "df.ix[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "linkage_matrix = ward(pos) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=titles);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#uncomment below to save figure\n",
    "plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bandwidth = estimate_bandwidth(pos, quantile=0.208)\n",
    "\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(pos)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "\n",
    "print(\"number of estimated clusters : %d\" % n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "plt.figure(1)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    my_members = labels == k\n",
    "    cluster_center = cluster_centers[k]\n",
    "    plt.plot(pos[my_members, 0], pos[my_members, 1], col + '.', markersize=12)\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=9)\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    plt.text(pos[i, 0], pos[i, 1], df.ix[i]['title'], size=11)\n",
    "\n",
    "df.ix[2]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDA AND NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def strip_proppers(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.islower()]\n",
    "    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#strip any proper nouns (NNP) or plural proper nouns (NNPS) from a text\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def strip_proppers_POS(text):\n",
    "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
    "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "    return non_propernouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        m = re.match('(\\w+)', token)\n",
    "        if m:\n",
    "            for c in m.groups():\n",
    "                filtered_tokens.append(c)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preprocess = [strip_proppers(doc) for doc in book_words_list] #remove proper names\n",
    "tokenized_text = [tokenize(text) for text in preprocess]#tokenize\n",
    "texts = [[word for word in text if word not in english_stopwords] for text in tokenized_text]#stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "#convert the dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus, \n",
    "                      num_topics=6 ,\n",
    "                            id2word=dictionary, \n",
    "                            update_every=6, \n",
    "                            chunksize=10000, \n",
    "                            passes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.show_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=6, random_state=1).fit(frequency_term_matrix)\n",
    "\n",
    "feature_names = term_freq_vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-5 - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
